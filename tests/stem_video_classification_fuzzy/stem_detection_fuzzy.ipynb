{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ecn8EsebWr5m"
   },
   "source": [
    "This data processing was done using Google Colab (to load file directly from Google Drive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j0_vuD5dXku0"
   },
   "source": [
    "### Getting the file from gdrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CoGWfpI9WrSA",
    "outputId": "cbb27a83-9451-434e-86cd-2b32edd50bbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iq06VM10XIYx",
    "outputId": "f6ebc148-4a07-4e0f-9473-d074313cf9db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_with_channelcountry.csv.zip\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/content/drive/MyDrive/edu-data\")\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yAQavUHrXQlP",
    "outputId": "a485dcf7-9032-4038-c4c8-cf017248f109"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /content/drive/MyDrive/edu-data/video_with_channelcountry.csv.zip\n",
      "  inflating: video_with_channelcountry.csv  \n",
      "  inflating: __MACOSX/._video_with_channelcountry.csv  \n"
     ]
    }
   ],
   "source": [
    "!unzip \"/content/drive/MyDrive/edu-data/video_with_channelcountry.csv.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xLq7JHOfXp2r"
   },
   "source": [
    "### Read file and get video data with country attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FHhyY8LyX1v7",
    "outputId": "c7f246bb-0885-4d2d-e2a7-c3a500c6fc8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rapidfuzz\n",
      "  Downloading rapidfuzz-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Downloading rapidfuzz-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: rapidfuzz\n",
      "Successfully installed rapidfuzz-3.10.1\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "! pip3 install rapidfuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "SPxFRxjCX1Ny"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from rapidfuzz import fuzz, process\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fhwHXZuLXhRy",
    "outputId": "d3a57e0e-8b5c-44eb-d324-d1baec6a0774"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-896327dcbe5a>:2: DtypeWarning: Columns (7,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_video_with_country = pd.read_csv('/content/drive/MyDrive/edu-data/video_with_channelcountry.csv')\n"
     ]
    }
   ],
   "source": [
    "# Extract video data with country field (removing those without any country information)\n",
    "df_video_with_country = pd.read_csv('/content/drive/MyDrive/edu-data/video_with_channelcountry.csv')\n",
    "df_video_with_country = df_video_with_country[df_video_with_country['country'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W8Qeq2wvYEKW",
    "outputId": "1a6e7cb4-10c8-486c-eca0-5baefeb024a6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1911894"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_video_with_country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5107n8U8Yqvl"
   },
   "source": [
    "So at this starting point we have around 2 million videos :o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fCxJYrWoYNhO",
    "outputId": "dcefa589-92dc-4849-9514-3f0704ab4724"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 93597  100 93597    0     0   159k      0 --:--:-- --:--:-- --:--:--  159k\n"
     ]
    }
   ],
   "source": [
    "# Get final STEM keywords list\n",
    "!curl -o final_keywords.txt https://raw.githubusercontent.com/epfl-ada/ada-2024-project-adacadabra2048/refs/heads/main/src/scripts/keyword_generator/final_keywords.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VU75kNnlYdH7"
   },
   "source": [
    "First we \"flatten\" the keyword hierarchy to extract the list of unique STEM keywords, which will later be used to match with our video tags for detecting STEM content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GGS2lYEFYSvo",
    "outputId": "c3df3206-338b-42e5-ada1-1a142424d5dc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2008"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_keywords(filepath):\n",
    "    \"\"\"\n",
    "    Extracts unique keywords from a file with the format \"x_keywords: [...]\".\n",
    "    \"\"\"\n",
    "    keywords = {}\n",
    "    with open(filepath, 'r') as file:\n",
    "        for line in file:\n",
    "            match = re.match(r\"(\\w+_keywords)\\s*=\\s*\\[(.*?)\\]\", line, re.DOTALL)\n",
    "            if match:\n",
    "                category = match.group(1)\n",
    "                keyword_str = match.group(2)\n",
    "                # Handle both single and double quotes, special characters,\n",
    "                # remove extra whitespace, and filter out empty strings\n",
    "                # The change is in the regex:\n",
    "                keyword_list = [''.join(k).strip().strip(\"'\").strip('\"').strip(\"’\") for k in re.findall(r\"'([^']*)'|[\\\"]([^\\\"]*)[\\\"]|’([^’]*)’\", keyword_str)]\n",
    "                keyword_list = [keyword for keyword in keyword_list if keyword]\n",
    "                keywords[category] = keyword_list\n",
    "\n",
    "    # Flatten the dictionary and remove duplicates\n",
    "    all_keywords = []\n",
    "    for category, wordlist in keywords.items():\n",
    "        all_keywords.extend(wordlist)\n",
    "\n",
    "    return list(set(all_keywords))\n",
    "\n",
    "stem_keywords = extract_keywords('final_keywords.txt')\n",
    "len(stem_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g7IOlnizaJqi",
    "outputId": "f0c76a4f-ccf1-4429-a76d-e9ece64a5566"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'python' found in stem_keywords\n",
      "'java' found in stem_keywords\n",
      "'circuits' found in stem_keywords\n",
      "'language' not found in stem_keywords\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check:\n",
    "keywords_to_check = [\"python\", \"java\", \"circuits\", \"language\"]\n",
    "for keyword in keywords_to_check:\n",
    "  if keyword.capitalize() in stem_keywords:\n",
    "    print(f\"'{keyword}' found in stem_keywords\")\n",
    "  else:\n",
    "    print(f\"'{keyword}' not found in stem_keywords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJ8ke1D8ayNU"
   },
   "source": [
    "Now we fuzzy match the tags of each video with our unique STEM keyword list that we've just constructed. We define a heuristic threshold of 50%, where a video with more than half of their tags being considered as STEM keywords will be considered as STEM videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IOvku1w8aPIh",
    "outputId": "deb0323b-6422-4d41-fc82-90503b10ae3d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing is_stem: 100%|██████████| 1911894/1911894 [1:48:15<00:00, 294.34it/s]\n",
      "100%|██████████| 56765/56765 [01:50<00:00, 512.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of STEM videos: 56765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Pre-process keywords (convert to lowercase)\n",
    "stem_keywords_lower = [keyword.lower() for keyword in stem_keywords]\n",
    "\n",
    "def is_stem_video(tags):\n",
    "    \"\"\"Checks if a video is STEM based on its tags.\"\"\"\n",
    "    if pd.isna(tags):\n",
    "        return False\n",
    "\n",
    "    video_tags = tags.lower().split(',')  # Convert tags to lowercase for efficient matching\n",
    "    matched_tags = 0\n",
    "    for video_tag in video_tags:\n",
    "        # Using rapidfuzz's process.extractOne for faster fuzzy matching\n",
    "        match = process.extractOne(video_tag.strip(), stem_keywords_lower, scorer=fuzz.ratio, score_cutoff=70)\n",
    "        if match:\n",
    "            matched_tags += 1\n",
    "\n",
    "    return matched_tags >= len(video_tags) / 2\n",
    "\n",
    "# Initialize keyword counts with lowercase keys\n",
    "keyword_counts = {keyword.lower(): 0 for keyword in stem_keywords}\n",
    "\n",
    "# Apply the is_stem_video function using Pandas apply for vectorization\n",
    "# with tqdm progress bar\n",
    "tqdm.pandas(desc=\"Processing is_stem\")\n",
    "df_video_with_country['is_stem'] = df_video_with_country['tags'].progress_apply(is_stem_video)\n",
    "\n",
    "# Update keyword counts (this part is still iterative, but it's unavoidable)\n",
    "for index, row in tqdm(df_video_with_country[df_video_with_country['is_stem']].iterrows(), total=len(df_video_with_country[df_video_with_country['is_stem']])):\n",
    "    video_tags = row['tags'].lower().split(',')\n",
    "    for video_tag in video_tags:\n",
    "        match = process.extractOne(video_tag.strip(), stem_keywords_lower, scorer=fuzz.ratio, score_cutoff=70)\n",
    "        if match:\n",
    "            keyword_counts[match[0]] += 1\n",
    "\n",
    "# Print the number of videos with is_stem = True\n",
    "stem_video_count = df_video_with_country['is_stem'].sum()\n",
    "print(f\"Number of STEM videos: {stem_video_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R4Mwiw1P5Av9"
   },
   "source": [
    "Out of around 2 million educational videos (with country data), more than 56,000 of them are considered STEM videos (~3%) by our detection method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "5aKp4wKI5uot"
   },
   "outputs": [],
   "source": [
    "# Save the DataFrame with the 'is_stem' column to a new CSV file\n",
    "df_video_with_country.to_csv('video_with_country_and_stem.csv', index=False)\n",
    "\n",
    "# Save the keyword counts to a text file\n",
    "with open('keyword_counts.txt', 'w') as f:\n",
    "    for keyword, count in keyword_counts.items():\n",
    "        f.write(f'{keyword},{count}\\n')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
