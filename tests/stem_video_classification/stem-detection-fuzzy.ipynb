{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This data processing was done using Google Colab (to load file directly from Google Drive)"
      ],
      "metadata": {
        "id": "Ecn8EsebWr5m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting the file from gdrive"
      ],
      "metadata": {
        "id": "j0_vuD5dXku0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoGWfpI9WrSA",
        "outputId": "cbb27a83-9451-434e-86cd-2b32edd50bbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive/edu-data\")\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iq06VM10XIYx",
        "outputId": "f6ebc148-4a07-4e0f-9473-d074313cf9db"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "video_with_channelcountry.csv.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/drive/MyDrive/edu-data/video_with_channelcountry.csv.zip\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAQavUHrXQlP",
        "outputId": "a485dcf7-9032-4038-c4c8-cf017248f109"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/edu-data/video_with_channelcountry.csv.zip\n",
            "  inflating: video_with_channelcountry.csv  \n",
            "  inflating: __MACOSX/._video_with_channelcountry.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read file and get video data with country attributes"
      ],
      "metadata": {
        "id": "xLq7JHOfXp2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "! pip3 install rapidfuzz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHhyY8LyX1v7",
        "outputId": "c7f246bb-0885-4d2d-e2a7-c3a500c6fc8e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rapidfuzz\n",
            "  Downloading rapidfuzz-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Downloading rapidfuzz-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz\n",
            "Successfully installed rapidfuzz-3.10.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from rapidfuzz import fuzz, process\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "SPxFRxjCX1Ny"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract video data with country field (removing those without any country information)\n",
        "df_video_with_country = pd.read_csv('/content/drive/MyDrive/edu-data/video_with_channelcountry.csv')\n",
        "df_video_with_country = df_video_with_country[df_video_with_country['country'].notnull()]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhwHXZuLXhRy",
        "outputId": "d3a57e0e-8b5c-44eb-d324-d1baec6a0774"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-896327dcbe5a>:2: DtypeWarning: Columns (7,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df_video_with_country = pd.read_csv('/content/drive/MyDrive/edu-data/video_with_channelcountry.csv')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(df_video_with_country)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8Qeq2wvYEKW",
        "outputId": "1a6e7cb4-10c8-486c-eca0-5baefeb024a6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1911894"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So at this starting point we have around 2 million videos :o"
      ],
      "metadata": {
        "id": "5107n8U8Yqvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get final STEM keywords list\n",
        "!curl -o final_keywords.txt https://raw.githubusercontent.com/epfl-ada/ada-2024-project-adacadabra2048/refs/heads/main/src/scripts/keyword_generator/final_keywords.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCxJYrWoYNhO",
        "outputId": "dcefa589-92dc-4849-9514-3f0704ab4724"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 93597  100 93597    0     0   159k      0 --:--:-- --:--:-- --:--:--  159k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we \"flatten\" the keyword hierarchy to extract the list of unique STEM keywords, which will later be used to match with our video tags for detecting STEM content."
      ],
      "metadata": {
        "id": "VU75kNnlYdH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_keywords(filepath):\n",
        "    \"\"\"\n",
        "    Extracts unique keywords from a file with the format \"x_keywords: [...]\".\n",
        "    \"\"\"\n",
        "    keywords = {}\n",
        "    with open(filepath, 'r') as file:\n",
        "        for line in file:\n",
        "            match = re.match(r\"(\\w+_keywords)\\s*=\\s*\\[(.*?)\\]\", line, re.DOTALL)\n",
        "            if match:\n",
        "                category = match.group(1)\n",
        "                keyword_str = match.group(2)\n",
        "                # Handle both single and double quotes, special characters,\n",
        "                # remove extra whitespace, and filter out empty strings\n",
        "                # The change is in the regex:\n",
        "                keyword_list = [''.join(k).strip().strip(\"'\").strip('\"').strip(\"’\") for k in re.findall(r\"'([^']*)'|[\\\"]([^\\\"]*)[\\\"]|’([^’]*)’\", keyword_str)]\n",
        "                keyword_list = [keyword for keyword in keyword_list if keyword]\n",
        "                keywords[category] = keyword_list\n",
        "\n",
        "    # Flatten the dictionary and remove duplicates\n",
        "    all_keywords = []\n",
        "    for category, wordlist in keywords.items():\n",
        "        all_keywords.extend(wordlist)\n",
        "\n",
        "    return list(set(all_keywords))\n",
        "\n",
        "stem_keywords = extract_keywords('final_keywords.txt')\n",
        "len(stem_keywords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGS2lYEFYSvo",
        "outputId": "c3df3206-338b-42e5-ada1-1a142424d5dc"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2008"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity Check:\n",
        "keywords_to_check = [\"python\", \"java\", \"circuits\", \"language\"]\n",
        "for keyword in keywords_to_check:\n",
        "  if keyword.capitalize() in stem_keywords:\n",
        "    print(f\"'{keyword}' found in stem_keywords\")\n",
        "  else:\n",
        "    print(f\"'{keyword}' not found in stem_keywords\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7IOlnizaJqi",
        "outputId": "f0c76a4f-ccf1-4429-a76d-e9ece64a5566"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'python' found in stem_keywords\n",
            "'java' found in stem_keywords\n",
            "'circuits' found in stem_keywords\n",
            "'language' not found in stem_keywords\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we fuzzy match the tags of each video with our unique STEM keyword list that we've just constructed. We define a heuristic threshold of 50%, where a video with more than half of their tags being considered as STEM keywords will be considered as STEM videos."
      ],
      "metadata": {
        "id": "cJ8ke1D8ayNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from rapidfuzz import fuzz, process\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Pre-process keywords (convert to lowercase)\n",
        "stem_keywords_lower = [keyword.lower() for keyword in stem_keywords]\n",
        "\n",
        "def is_stem_video(tags):\n",
        "    \"\"\"Checks if a video is STEM based on its tags.\"\"\"\n",
        "    if pd.isna(tags):\n",
        "        return False\n",
        "\n",
        "    video_tags = tags.lower().split(',')  # Convert tags to lowercase for efficient matching\n",
        "    matched_tags = 0\n",
        "    for video_tag in video_tags:\n",
        "        # Using rapidfuzz's process.extractOne for faster fuzzy matching\n",
        "        match = process.extractOne(video_tag.strip(), stem_keywords_lower, scorer=fuzz.ratio, score_cutoff=70)\n",
        "        if match:\n",
        "            matched_tags += 1\n",
        "\n",
        "    return matched_tags >= len(video_tags) / 2\n",
        "\n",
        "# Initialize keyword counts with lowercase keys\n",
        "keyword_counts = {keyword.lower(): 0 for keyword in stem_keywords}\n",
        "\n",
        "# Apply the is_stem_video function using Pandas apply for vectorization\n",
        "# with tqdm progress bar\n",
        "tqdm.pandas(desc=\"Processing is_stem\")\n",
        "df_video_with_country['is_stem'] = df_video_with_country['tags'].progress_apply(is_stem_video)\n",
        "\n",
        "# Update keyword counts (this part is still iterative, but it's unavoidable)\n",
        "for index, row in tqdm(df_video_with_country[df_video_with_country['is_stem']].iterrows(), total=len(df_video_with_country[df_video_with_country['is_stem']])):\n",
        "    video_tags = row['tags'].lower().split(',')\n",
        "    for video_tag in video_tags:\n",
        "        match = process.extractOne(video_tag.strip(), stem_keywords_lower, scorer=fuzz.ratio, score_cutoff=70)\n",
        "        if match:\n",
        "            keyword_counts[match[0]] += 1\n",
        "\n",
        "# Print the number of videos with is_stem = True\n",
        "stem_video_count = df_video_with_country['is_stem'].sum()\n",
        "print(f\"Number of STEM videos: {stem_video_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOvku1w8aPIh",
        "outputId": "deb0323b-6422-4d41-fc82-90503b10ae3d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing is_stem: 100%|██████████| 1911894/1911894 [1:48:15<00:00, 294.34it/s]\n",
            "100%|██████████| 56765/56765 [01:50<00:00, 512.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of STEM videos: 56765\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Out of around 2 million educational videos (with country data), more than 56,000 of them are considered STEM videos (~3%) by our detection method."
      ],
      "metadata": {
        "id": "R4Mwiw1P5Av9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the DataFrame with the 'is_stem' column to a new CSV file\n",
        "df_video_with_country.to_csv('video_with_country_and_stem.csv', index=False)\n",
        "\n",
        "# Save the keyword counts to a text file\n",
        "with open('keyword_counts.txt', 'w') as f:\n",
        "    for keyword, count in keyword_counts.items():\n",
        "        f.write(f'{keyword},{count}\\n')"
      ],
      "metadata": {
        "id": "5aKp4wKI5uot"
      },
      "execution_count": 26,
      "outputs": []
    }
  ]
}